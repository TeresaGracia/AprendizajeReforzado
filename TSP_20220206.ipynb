{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TSP_20220206.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPpCpS65Pnzmi7AgYk2aQOV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TeresaGracia/AprendizajeReforzado/blob/main/TSP_20220206.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYbkWqg0skv3",
        "outputId": "4742333d-756d-47ca-c660-b5aefca2be57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: or_gym in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from or_gym) (2.6.3)\n",
            "Requirement already satisfied: numpy>=1.16.1 in /usr/local/lib/python3.7/dist-packages (from or_gym) (1.19.5)\n",
            "Requirement already satisfied: matplotlib>=3.1 in /usr/local/lib/python3.7/dist-packages (from or_gym) (3.2.2)\n",
            "Requirement already satisfied: gym>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from or_gym) (0.17.3)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from or_gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15.0->or_gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15.0->or_gym) (1.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->or_gym) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->or_gym) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->or_gym) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->or_gym) (1.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.15.0->or_gym) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1->or_gym) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install or_gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementación Reinforce\n",
        "import numpy as np\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "import or_gym"
      ],
      "metadata": {
        "id": "dllCgqeXsrWa"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creación red neuronal\n",
        "env=or_gym.make('TSP-v1')\n",
        "env.reset()\n",
        "obs_size = env.observation_space.shape[0] \n",
        "n_actions = env.action_space.n  \n",
        "HIDDEN_SIZE = 256\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "             torch.nn.Linear(obs_size, HIDDEN_SIZE),\n",
        "             torch.nn.ReLU(),\n",
        "             torch.nn.Linear(HIDDEN_SIZE, n_actions),\n",
        "             torch.nn.Softmax(dim=0)\n",
        "     )\n"
      ],
      "metadata": {
        "id": "LIiprRtFswRM"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ascenso del gradiente\n",
        "LEARNING_RATE = 0.0018\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "8X7r_IDtszeE"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bucle de entrenamiento\n",
        "HORIZON = 5000\n",
        "MAX_TRAJECTORIES =2500\n",
        "GAMMA = 1\n",
        "\n",
        "score = [] \n",
        "for trajectory in range(MAX_TRAJECTORIES):\n",
        "    current_state = env.reset()\n",
        "    done = False\n",
        "    transitions = [] \n",
        "    reward_transitions = [] \n",
        "    reward_batch=[] \n",
        "\n",
        "    for t in range(HORIZON):\n",
        "        actions_prob = model(torch.from_numpy(current_state).float())\n",
        "        p=actions_prob.data.numpy()\n",
        "        longitud=len(p)\n",
        "        a=[]\n",
        "        for j in range(int(env.N)):#Para obtener la dimensión del espacio acción\n",
        "          a.append(j)        \n",
        "        action = np.random.choice(a,p=actions_prob.data.numpy()) \n",
        "        contador=0\n",
        "        while current_state[action+1]==1:\n",
        "          action=np.random.choice(a,p=actions_prob.data.numpy()) \n",
        "          if contador==50:\n",
        "            break\n",
        "          else:\n",
        "            contador=contador+1        \n",
        "        previous_state = current_state\n",
        "        current_state, reward, done, info = env.step(action) \n",
        "        if reward!=-100 and reward!=1000: #Para que la recompensa sea mayor a menor distancia\n",
        "          reward=1-reward\n",
        "        transitions.append((previous_state, action ,t+1)) \n",
        "        reward_transitions.append((reward)) \n",
        "        if done: \n",
        "            break\n",
        "    score.append(len(transitions))\n",
        "    batch_Gvals =[]\n",
        "    power=0\n",
        "    new_Gval=0\n",
        "    for recompensa in reward_transitions:\n",
        "      new_Gval=new_Gval+((GAMMA**power)*recompensa)\n",
        "      power+=1\n",
        "    batch_Gvals.append(new_Gval)\n",
        "    expected_returns_batch=torch.FloatTensor(batch_Gvals)  \n",
        "    expected_returns_batch /= expected_returns_batch.max()\n",
        "\n",
        "    state_batch = torch.Tensor([s for (s,a,r) in transitions]) \n",
        "    action_batch = torch.Tensor([a for (s,a,r) in transitions]) \n",
        "\n",
        "    predicted_batch = model(state_batch) \n",
        "    prob_batch = predicted_batch.gather(dim=1,index=action_batch.long().view(-1,1)).squeeze() #Creo que esto es lo que está mal\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    loss = - torch.sum(torch.log(prob_batch) * expected_returns_batch) \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if trajectory % 100 == 0 and trajectory>0:\n",
        "        print('Trajectory {}\\tAverage Score: {:.2f}'.format(trajectory, np.mean(score[-50:-1])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7EbqsTms2IE",
        "outputId": "6c6d930d-370a-49aa-c839-6176e2ff7fbc"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trajectory 100\tAverage Score: 45.98\n",
            "Trajectory 200\tAverage Score: 47.37\n",
            "Trajectory 300\tAverage Score: 47.47\n",
            "Trajectory 400\tAverage Score: 48.16\n",
            "Trajectory 500\tAverage Score: 47.78\n",
            "Trajectory 600\tAverage Score: 48.65\n",
            "Trajectory 700\tAverage Score: 48.96\n",
            "Trajectory 800\tAverage Score: 48.94\n",
            "Trajectory 900\tAverage Score: 48.94\n",
            "Trajectory 1000\tAverage Score: 48.84\n",
            "Trajectory 1100\tAverage Score: 48.69\n",
            "Trajectory 1200\tAverage Score: 48.45\n",
            "Trajectory 1300\tAverage Score: 49.00\n",
            "Trajectory 1400\tAverage Score: 49.00\n",
            "Trajectory 1500\tAverage Score: 48.96\n",
            "Trajectory 1600\tAverage Score: 49.00\n",
            "Trajectory 1700\tAverage Score: 49.00\n",
            "Trajectory 1800\tAverage Score: 49.00\n",
            "Trajectory 1900\tAverage Score: 49.00\n",
            "Trajectory 2000\tAverage Score: 49.00\n",
            "Trajectory 2100\tAverage Score: 49.00\n",
            "Trajectory 2200\tAverage Score: 49.00\n",
            "Trajectory 2300\tAverage Score: 49.00\n",
            "Trajectory 2400\tAverage Score: 49.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Aplicación de la red neuronal para resolver el entorno\n",
        "\n",
        "def watch_agent():\n",
        "  env = or_gym.make('TSP-v1')\n",
        "  state = env.reset()\n",
        "  rewards = []\n",
        "  win=0\n",
        "  lost=0\n",
        "  total=0\n",
        "  max_win=0\n",
        "  max_lost=-100\n",
        "  for i in range(50000):\n",
        "    rewards=[]\n",
        "    state = env.reset()\n",
        "    for t in range(2000):\n",
        "      pred = model(torch.from_numpy(state).float())\n",
        "      action = np.random.choice(a, p=pred.data.numpy())\n",
        "      state, reward, done, _ = env.step(action)\n",
        "      if reward!= -100 and reward!=1000:\n",
        "        reward=1-reward\n",
        "      rewards.append(reward)\n",
        "      if done:\n",
        "          #print(state)\n",
        "          #print(\"Reward:\", sum([r for r in rewards]))\n",
        "          total_reward=sum([r for r in rewards])\n",
        "          if total_reward>0:\n",
        "            win=win+1\n",
        "            total=total+1\n",
        "            if total_reward>max_win:\n",
        "              max_win=total_reward\n",
        "          else:\n",
        "            lost=lost+1\n",
        "            total=total+1\n",
        "            if total_reward>max_lost:\n",
        "              max_lost=total_reward\n",
        "          break \n",
        "  env.close()\n",
        "  print(\"Completados\",win)\n",
        "  print(\"Perdidos\",lost)\n",
        "  print(\"Total\", total)\n",
        "  print(\"Porcentaje completados\",100*win/total)\n",
        "  print(\"Máxima recompensa en los completados\",max_win)\n",
        "  print(\"Máxima recompensa en los perdidos\",max_lost)\n",
        "watch_agent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtOHdqwmtIJM",
        "outputId": "e0d09a2c-98aa-4315-8cc2-9da05719dad5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completados 41876\n",
            "Perdidos 8124\n",
            "Total 50000\n",
            "Porcentaje completados 83.752\n",
            "Máxima recompensa en los completados 29.788704032718424\n",
            "Máxima recompensa en los perdidos -71.95193091601143\n"
          ]
        }
      ]
    }
  ]
}